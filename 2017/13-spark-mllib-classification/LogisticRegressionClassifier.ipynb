{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility function to create the appropriate data frame for classification algorithms in MLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapLibSVM(row): \n",
    "    return (row[5],Vectors.dense(row[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create the dataframe from a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "        .format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .load(\"datasets/iris.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-----------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|      label|\n",
      "+------------+-----------+------------+-----------+-----------+\n",
      "|         5.1|        3.5|         1.4|        0.2|Iris-setosa|\n",
      "|         4.9|        3.0|         1.4|        0.2|Iris-setosa|\n",
      "|         4.7|        3.2|         1.3|        0.2|Iris-setosa|\n",
      "|         4.6|        3.1|         1.5|        0.2|Iris-setosa|\n",
      "|         5.0|        3.6|         1.4|        0.2|Iris-setosa|\n",
      "|         5.4|        3.9|         1.7|        0.4|Iris-setosa|\n",
      "|         4.6|        3.4|         1.4|        0.3|Iris-setosa|\n",
      "|         5.0|        3.4|         1.5|        0.2|Iris-setosa|\n",
      "|         4.4|        2.9|         1.4|        0.2|Iris-setosa|\n",
      "|         4.9|        3.1|         1.5|        0.1|Iris-setosa|\n",
      "|         5.4|        3.7|         1.5|        0.2|Iris-setosa|\n",
      "|         4.8|        3.4|         1.6|        0.2|Iris-setosa|\n",
      "|         4.8|        3.0|         1.4|        0.1|Iris-setosa|\n",
      "|         4.3|        3.0|         1.1|        0.1|Iris-setosa|\n",
      "|         5.8|        4.0|         1.2|        0.2|Iris-setosa|\n",
      "|         5.7|        4.4|         1.5|        0.4|Iris-setosa|\n",
      "|         5.4|        3.9|         1.3|        0.4|Iris-setosa|\n",
      "|         5.1|        3.5|         1.4|        0.3|Iris-setosa|\n",
      "|         5.7|        3.8|         1.7|        0.3|Iris-setosa|\n",
      "|         5.1|        3.8|         1.5|        0.3|Iris-setosa|\n",
      "+------------+-----------+------------+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification algorithms requires numeric values for labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-----------+----------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|      label|labelIndex|\n",
      "+------------+-----------+------------+-----------+-----------+----------+\n",
      "|         5.1|        3.5|         1.4|        0.2|Iris-setosa|       0.0|\n",
      "|         4.9|        3.0|         1.4|        0.2|Iris-setosa|       0.0|\n",
      "|         4.7|        3.2|         1.3|        0.2|Iris-setosa|       0.0|\n",
      "|         4.6|        3.1|         1.5|        0.2|Iris-setosa|       0.0|\n",
      "|         5.0|        3.6|         1.4|        0.2|Iris-setosa|       0.0|\n",
      "|         5.4|        3.9|         1.7|        0.4|Iris-setosa|       0.0|\n",
      "|         4.6|        3.4|         1.4|        0.3|Iris-setosa|       0.0|\n",
      "|         5.0|        3.4|         1.5|        0.2|Iris-setosa|       0.0|\n",
      "|         4.4|        2.9|         1.4|        0.2|Iris-setosa|       0.0|\n",
      "|         4.9|        3.1|         1.5|        0.1|Iris-setosa|       0.0|\n",
      "|         5.4|        3.7|         1.5|        0.2|Iris-setosa|       0.0|\n",
      "|         4.8|        3.4|         1.6|        0.2|Iris-setosa|       0.0|\n",
      "|         4.8|        3.0|         1.4|        0.1|Iris-setosa|       0.0|\n",
      "|         4.3|        3.0|         1.1|        0.1|Iris-setosa|       0.0|\n",
      "|         5.8|        4.0|         1.2|        0.2|Iris-setosa|       0.0|\n",
      "|         5.7|        4.4|         1.5|        0.4|Iris-setosa|       0.0|\n",
      "|         5.4|        3.9|         1.3|        0.4|Iris-setosa|       0.0|\n",
      "|         5.1|        3.5|         1.4|        0.3|Iris-setosa|       0.0|\n",
      "|         5.7|        3.8|         1.7|        0.3|Iris-setosa|       0.0|\n",
      "|         5.1|        3.8|         1.5|        0.3|Iris-setosa|       0.0|\n",
      "+------------+-----------+------------+-----------+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexer = StringIndexer(inputCol=\"label\", outputCol=\"labelIndex\")\n",
    "indexer = indexer.fit(df).transform(df)\n",
    "indexer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+\n",
      "|label|     features|\n",
      "+-----+-------------+\n",
      "|  0.0|[5.1,3.5,1.4]|\n",
      "|  0.0|[4.9,3.0,1.4]|\n",
      "|  0.0|[4.7,3.2,1.3]|\n",
      "|  0.0|[4.6,3.1,1.5]|\n",
      "|  0.0|[5.0,3.6,1.4]|\n",
      "|  0.0|[5.4,3.9,1.7]|\n",
      "|  0.0|[4.6,3.4,1.4]|\n",
      "|  0.0|[5.0,3.4,1.5]|\n",
      "|  0.0|[4.4,2.9,1.4]|\n",
      "|  0.0|[4.9,3.1,1.5]|\n",
      "|  0.0|[5.4,3.7,1.5]|\n",
      "|  0.0|[4.8,3.4,1.6]|\n",
      "|  0.0|[4.8,3.0,1.4]|\n",
      "|  0.0|[4.3,3.0,1.1]|\n",
      "|  0.0|[5.8,4.0,1.2]|\n",
      "|  0.0|[5.7,4.4,1.5]|\n",
      "|  0.0|[5.4,3.9,1.3]|\n",
      "|  0.0|[5.1,3.5,1.4]|\n",
      "|  0.0|[5.7,3.8,1.7]|\n",
      "|  0.0|[5.1,3.8,1.5]|\n",
      "+-----+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfLabeled = indexer.rdd.map(mapLibSVM).toDF([\"label\", \"features\"])\n",
    "dfLabeled.show()\n",
    "train, test = dfLabeled.randomSplit([0.9, 0.1], seed=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "schema verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the Logistic Regression and the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(labelCol=\"label\", maxIter=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a *ParamGridBuilder* to construct a grid of parameters to search over.\n",
    "\n",
    "*TrainValidationSplit* will try all combinations of values and determine best model using the evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(lr.regParam, [0.1, 0.001]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the estimator is simply the linear regression.\n",
    "\n",
    "A TrainValidationSplit requires an Estimator, a set of Estimator ParamMaps, and an Evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvs = TrainValidationSplit(estimator=lr,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=RegressionEvaluator(),\n",
    "                           # 80% of the data will be used for training, 20% for validation.\n",
    "                           trainRatio=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the pipeline to training documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tvs.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the predictions from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|label|\n",
      "+----------+-----+\n",
      "|       0.0|  0.0|\n",
      "|       0.0|  0.0|\n",
      "|       0.0|  0.0|\n",
      "|       0.0|  0.0|\n",
      "|       0.0|  0.0|\n",
      "|       1.0|  1.0|\n",
      "|       1.0|  1.0|\n",
      "|       1.0|  1.0|\n",
      "|       2.0|  2.0|\n",
      "|       2.0|  2.0|\n",
      "|       2.0|  2.0|\n",
      "|       2.0|  2.0|\n",
      "|       2.0|  2.0|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = model.transform(test)\n",
    "predictions = result.select([\"prediction\", \"label\"])\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Stats\n",
      "/home/marcosfe/spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/evaluation.py:237: UserWarning: Deprecated in 2.0.0. Use accuracy.\n",
      "  warnings.warn(\"Deprecated in 2.0.0. Use accuracy.\")\n",
      "Precision = 1.0\n",
      "/home/marcosfe/spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/evaluation.py:249: UserWarning: Deprecated in 2.0.0. Use accuracy.\n",
      "  warnings.warn(\"Deprecated in 2.0.0. Use accuracy.\")\n",
      "Recall = 1.0\n",
      "/home/marcosfe/spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/evaluation.py:262: UserWarning: Deprecated in 2.0.0. Use accuracy.\n",
      "  warnings.warn(\"Deprecated in 2.0.0. Use accuracy.\")\n",
      "F1 Score = 1.0\n",
      "Accuracy = 1.0\n"
     ]
    }
   ],
   "source": [
    "# Instantiate metrics object\n",
    "metrics = MulticlassMetrics(predictions.rdd)\n",
    "\n",
    "# Overall statistics\n",
    "print(\"Summary Stats\")\n",
    "print(\"Precision = %s\" % metrics.precision())\n",
    "print(\"Recall = %s\" % metrics.recall())\n",
    "print(\"F1 Score = %s\" % metrics.fMeasure())\n",
    "print(\"Accuracy = %s\" % metrics.accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted recall = 1.0\n",
      "Weighted precision = 1.0\n",
      "Weighted F(1) Score = 1.0\n",
      "Weighted F(0.5) Score = 1.0\n",
      "Weighted false positive rate = 0.0\n"
     ]
    }
   ],
   "source": [
    "# Weighted stats\n",
    "print(\"Weighted recall = %s\" % metrics.weightedRecall)\n",
    "print(\"Weighted precision = %s\" % metrics.weightedPrecision)\n",
    "print(\"Weighted F(1) Score = %s\" % metrics.weightedFMeasure())\n",
    "print(\"Weighted F(0.5) Score = %s\" % metrics.weightedFMeasure(beta=0.5))\n",
    "print(\"Weighted false positive rate = %s\" % metrics.weightedFalsePositiveRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - PySpark",
   "language": "python",
   "name": "apache_toree_pyspark"
  },
  "language_info": {
   "codemirror_mode": "text/x-ipython",
   "file_extension": ".py",
   "mimetype": "text/x-ipython",
   "name": "python",
   "pygments_lexer": "python",
   "version": "3.6.3\n"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
